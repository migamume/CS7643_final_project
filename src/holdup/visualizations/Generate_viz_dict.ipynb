{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6f8330cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd '/Users/pc/Desktop/Deep Learning/Project Github/CS7643_final_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "810a4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python src/holdup/datapickler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a00fdc",
   "metadata": {},
   "source": [
    "# Notebook consists of two parts\n",
    "# 1) Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66578102",
   "metadata": {},
   "source": [
    "# 2) Generating dictionaries used for visualization\n",
    "\n",
    "## Run code below for functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d080c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preflop_train_data_size: 30650\n",
      "preflop_test_data_size: 20434\n",
      "flop_train_data_size: 23450\n",
      "flop_test_data_size: 15634\n",
      "turn_train_data_size: 18447\n",
      "turn_test_data_size: 12298\n",
      "river_train_data_size: 16371\n",
      "river_test_data_size: 10915\n"
     ]
    }
   ],
   "source": [
    "from holdup.the_model.autoencoder import Autoencoder\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "from holdup.parser.replayable_hand import ReplayableHand, Streets\n",
    "import functools\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "from holdup.the_model.get_datasets import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "preflop = \"preflop\"\n",
    "flop = \"flop\"\n",
    "turn = \"turn\"\n",
    "river = \"river\"\n",
    "\n",
    "def get_stage(dataset, stage):\n",
    "    if stage == preflop:\n",
    "        return dataset[0]\n",
    "    if stage == flop:\n",
    "        return dataset[1]\n",
    "    if stage == turn:\n",
    "        return dataset[2]\n",
    "    if stage == river:\n",
    "        return dataset[3]\n",
    "\n",
    "def flatten_streets(dataset):\n",
    "    streets = [[], [], [], []]\n",
    "    for logfile in dataset:\n",
    "        for index, street in enumerate(logfile):\n",
    "            streets[index] = streets[index] + street\n",
    "    return streets\n",
    "\n",
    "\n",
    "def get_data(dataset, stage):\n",
    "    flattened_data = flatten_streets(dataset)\n",
    "    stage_data = get_stage(flattened_data, stage)\n",
    "    return [(x[0], x[1][1]) for x in stage_data]\n",
    "\n",
    "\n",
    "with open('last_possible.pickle', 'rb') as last_possible_pickle:\n",
    "    last_possible_dataset = pickle.load(last_possible_pickle)\n",
    "\n",
    "preflop_data = get_data(last_possible_dataset, \"preflop\")\n",
    "flop_data = get_data(last_possible_dataset, \"flop\")\n",
    "turn_data = get_data(last_possible_dataset, \"turn\")\n",
    "river_data = get_data(last_possible_dataset, \"river\")\n",
    "\n",
    "def separate_train_test(street_data):\n",
    "    n_train = int(len(street_data)*0.6)\n",
    "    train_set = street_data[:n_train]\n",
    "    test_set = street_data[n_train:]\n",
    "    return train_set,test_set\n",
    "\n",
    "train_preflop, test_preflop =separate_train_test(preflop_data)\n",
    "print(\"preflop_train_data_size: {}\".format(len(train_preflop)))\n",
    "print(\"preflop_test_data_size: {}\".format(len(test_preflop)))\n",
    "\n",
    "train_flop, test_flop=separate_train_test(flop_data)\n",
    "print(\"flop_train_data_size: {}\".format(len(train_flop)))\n",
    "print(\"flop_test_data_size: {}\".format(len(test_flop)))\n",
    "\n",
    "train_turn, test_turn=separate_train_test(turn_data)\n",
    "print(\"turn_train_data_size: {}\".format(len(train_turn)))\n",
    "print(\"turn_test_data_size: {}\".format(len(test_turn)))\n",
    "\n",
    "train_river, test_river=separate_train_test(river_data)\n",
    "print(\"river_train_data_size: {}\".format(len(train_river)))\n",
    "print(\"river_test_data_size: {}\".format(len(test_river)))\n",
    "\n",
    "\n",
    "# Set the device to use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create an instance of the autoencoder\n",
    "# model = Autoencoder(num_hidden_nodes).to(device)\n",
    "\n",
    "def train2(learning_rate, model, train_loader, num_epochs, wd, pftr='stage_name'):\n",
    "    criterion = nn.CrossEntropyLoss() #changed to cross entropy loss for classification based tasks (semi-supervised)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate, weight_decay=wd)\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            optimizer.zero_grad()\n",
    "            batch_size, _, _ = inputs.size()\n",
    "            inputs = inputs.view(batch_size, -1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "\n",
    "    \n",
    "def quick_test2(model,test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            batch_size, _, _ = inputs.size()\n",
    "            inputs = inputs.view(batch_size, -1)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return round((correct/total)*100, 2)\n",
    "\n",
    "\n",
    "def train_and_quick_test(lr, num_hidden_nodes, num_epochs, weight_decay,train_data,test_data, pftr):\n",
    "    # Define the model\n",
    "    model = Autoencoder(num_hidden_nodes).to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=20, shuffle=False)\n",
    "    train2(lr, model, train_loader, num_epochs, weight_decay,pftr)\n",
    "    # Test the model\n",
    "    quick_test(model,test_loader)\n",
    "    \n",
    "\n",
    "###\n",
    "\n",
    "def tune_model(learning_rate, weight_decay, num_epochs, train, test, pftr, hidden_nodes, batch_size):\n",
    "        lr = {}\n",
    "        for i in range(len(learning_rate)):\n",
    "            lr[str(learning_rate[i])] = []\n",
    "            for j in range(len(weight_decay)):\n",
    "                model = Autoencoder(hidden_nodes).to(device)\n",
    "                train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "                test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "                train2(learning_rate[i], model, train_loader, num_epochs, weight_decay[j],pftr)\n",
    "                result = quick_test2(model,test_loader)\n",
    "                lr[str(learning_rate[i])].append(result)\n",
    "        return lr\n",
    "    \n",
    "def get_nodes_dict(num_experiments, max_nodes, learning_rate, weight_decay, num_epochs, train, test, pftr, batch_size):\n",
    "        nd = {}\n",
    "        for i in range(10, max_nodes+10, 10):\n",
    "            nd[str(i)] = []\n",
    "            for j in range(num_experiments):\n",
    "                model = Autoencoder(i).to(device)\n",
    "                train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "                test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "                train2(learning_rate, model, train_loader, num_epochs, weight_decay,pftr)\n",
    "                result = quick_test2(model,test_loader)\n",
    "                nd[str(i)].append(result)\n",
    "            nd[str(i)] = tuple(nd[str(i)])\n",
    "        return nd\n",
    "    \n",
    "\n",
    "def get_epochs_dict(num_experiments, nodes, learning_rate, weight_decay, max_epochs, train, test, pftr, batch_size):\n",
    "        nd = {}\n",
    "        for i in range(10, max_epochs+10, 10):\n",
    "            nd[str(i)] = []\n",
    "            for j in range(num_experiments):\n",
    "                model = Autoencoder(nodes).to(device)\n",
    "                train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "                test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "                train2(learning_rate, model, train_loader, i, weight_decay,pftr)\n",
    "                result = quick_test2(model,test_loader)\n",
    "                nd[str(i)].append(result)\n",
    "            nd[str(i)] = tuple(nd[str(i)])\n",
    "        return nd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ee77e",
   "metadata": {},
   "source": [
    "# 1) Tuning the model\n",
    "\n",
    "## Best Model Parameters: \n",
    "\n",
    "### Learning Rate: .01 \n",
    "### Weight Decay: 0 (as it decreases it improves test accuracy)\n",
    "### Batch Size: 20 (Using a batch size of 20 as default as batch size doesn't have significant impact on model performance)\n",
    "\n",
    "## Code below is used to see the test accuracy obtained by using different learning rates with different weight decays. It can be seen that using a learning rate of .01 gave the best performance out of all of the models and that by decreasing weight decay the model works better and for that reason we opted to using a weight decay of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "5d427a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [.0001, .001, .01, .1]\n",
    "wd = [0, .0001, .1]\n",
    "\n",
    "pre_flop = tune_model(lr, wd, 20, train_preflop, test_preflop, pftr='preflop_last_possible', hidden_nodes=20, batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "71f94b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE FLOP\n",
      "learning rate: 0.0001\n",
      "     weight decay: 0 accuracy: 79.73\n",
      "     weight decay: 0.0001 accuracy: 79.55\n",
      "     weight decay: 0.1 accuracy: 45.97\n",
      "learning rate: 0.001\n",
      "     weight decay: 0 accuracy: 81.1\n",
      "     weight decay: 0.0001 accuracy: 80.52\n",
      "     weight decay: 0.1 accuracy: 45.97\n",
      "learning rate: 0.01\n",
      "     weight decay: 0 accuracy: 82.95\n",
      "     weight decay: 0.0001 accuracy: 80.41\n",
      "     weight decay: 0.1 accuracy: 45.97\n",
      "learning rate: 0.1\n",
      "     weight decay: 0 accuracy: 81.47\n",
      "     weight decay: 0.0001 accuracy: 78.99\n",
      "     weight decay: 0.1 accuracy: 30.58\n"
     ]
    }
   ],
   "source": [
    "pre_flop\n",
    "\n",
    "print(\"PRE FLOP\")\n",
    "for key, value in pre_flop.items():\n",
    "    print(\"learning rate: {}\".format(key))\n",
    "    for j in range(len(value)):\n",
    "        print(\"     weight decay: {}\".format(wd[j]), \"accuracy: {}\".format(value[j]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9f463302",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [.0001, .001, .01, .1]\n",
    "wd = [0, .0001, .1]\n",
    "\n",
    "flop = tune_model(lr, wd, 20, train_flop, test_flop, pftr='flop_last_possible', hidden_nodes=20, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "bab205ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOP\n",
      "learning rate: 0.0001\n",
      "     weight decay: 0 accuracy: 81.83\n",
      "     weight decay: 0.0001 accuracy: 82.05\n",
      "     weight decay: 0.1 accuracy: 56.77\n",
      "learning rate: 0.001\n",
      "     weight decay: 0 accuracy: 83.8\n",
      "     weight decay: 0.0001 accuracy: 83.16\n",
      "     weight decay: 0.1 accuracy: 56.77\n",
      "learning rate: 0.01\n",
      "     weight decay: 0 accuracy: 85.93\n",
      "     weight decay: 0.0001 accuracy: 83.91\n",
      "     weight decay: 0.1 accuracy: 56.77\n",
      "learning rate: 0.1\n",
      "     weight decay: 0 accuracy: 83.19\n",
      "     weight decay: 0.0001 accuracy: 77.2\n",
      "     weight decay: 0.1 accuracy: 56.77\n"
     ]
    }
   ],
   "source": [
    "print(\"FLOP\")\n",
    "for key, value in flop.items():\n",
    "    print(\"learning rate: {}\".format(key))\n",
    "    for j in range(len(value)):\n",
    "        print(\"     weight decay: {}\".format(wd[j]), \"accuracy: {}\".format(value[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "8ee92676",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [.0001, .001, .01, .1]\n",
    "wd = [0, .0001, .1]\n",
    "\n",
    "turn = tune_model(lr, wd, 40, train_turn, test_turn, pftr='turn_last_possible', hidden_nodes=40, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "971c0ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TURN\n",
      "learning rate: 0.0001\n",
      "     weight decay: 0 accuracy: 90.36\n",
      "     weight decay: 0.0001 accuracy: 90.45\n",
      "     weight decay: 0.1 accuracy: 68.31\n",
      "learning rate: 0.001\n",
      "     weight decay: 0 accuracy: 91.14\n",
      "     weight decay: 0.0001 accuracy: 90.61\n",
      "     weight decay: 0.1 accuracy: 68.31\n",
      "learning rate: 0.01\n",
      "     weight decay: 0 accuracy: 91.39\n",
      "     weight decay: 0.0001 accuracy: 90.5\n",
      "     weight decay: 0.1 accuracy: 68.31\n",
      "learning rate: 0.1\n",
      "     weight decay: 0 accuracy: 89.57\n",
      "     weight decay: 0.0001 accuracy: 87.92\n",
      "     weight decay: 0.1 accuracy: 68.31\n"
     ]
    }
   ],
   "source": [
    "print(\"TURN\")\n",
    "for key, value in turn.items():\n",
    "    print(\"learning rate: {}\".format(key))\n",
    "    for j in range(len(value)):\n",
    "        print(\"     weight decay: {}\".format(wd[j]), \"accuracy: {}\".format(value[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "330c0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [.0001, .001, .01, .1]\n",
    "wd = [0, .0001, .1]\n",
    "\n",
    "river = tune_model(lr, wd, 40, train_river, test_river, pftr='river_last_possible', hidden_nodes=40, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "13751422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIVER\n",
      "learning rate: 0.0001\n",
      "     weight decay: 0 accuracy: 87.36\n",
      "     weight decay: 0.0001 accuracy: 87.35\n",
      "     weight decay: 0.1 accuracy: 59.06\n",
      "learning rate: 0.001\n",
      "     weight decay: 0 accuracy: 89.33\n",
      "     weight decay: 0.0001 accuracy: 89.4\n",
      "     weight decay: 0.1 accuracy: 59.06\n",
      "learning rate: 0.01\n",
      "     weight decay: 0 accuracy: 89.68\n",
      "     weight decay: 0.0001 accuracy: 88.52\n",
      "     weight decay: 0.1 accuracy: 59.06\n",
      "learning rate: 0.1\n",
      "     weight decay: 0 accuracy: 87.66\n",
      "     weight decay: 0.0001 accuracy: 85.32\n",
      "     weight decay: 0.1 accuracy: 59.06\n"
     ]
    }
   ],
   "source": [
    "print(\"RIVER\")\n",
    "for key, value in river.items():\n",
    "    print(\"learning rate: {}\".format(key))\n",
    "    for j in range(len(value)):\n",
    "        print(\"     weight decay: {}\".format(wd[j]), \"accuracy: {}\".format(value[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922dd49",
   "metadata": {},
   "source": [
    "## Code Below is used to demonstrate how changing the batch size does not make significant change to improving the test accuracy of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "88333354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE FLOP\n",
      "Using learning rate of 0.01 and weight decay of 0:\n",
      "     When using batch size of 8, accuracy is 83\n",
      "     When using batch size of 16, accuracy is 83\n",
      "     When using batch size of 32, accuracy is 83\n",
      "     When using batch size of 64, accuracy is 83\n",
      "     When using batch size of 128, accuracy is 83\n",
      "     When using batch size of 256, accuracy is 83\n",
      "     When using batch size of 512, accuracy is 82\n",
      "     When using batch size of 1024, accuracy is 81\n"
     ]
    }
   ],
   "source": [
    "lr = [.01]\n",
    "wd = [0]\n",
    "b_s = [8,16,32,64,128,256,512,1024]\n",
    "\n",
    "ls = []\n",
    "\n",
    "for z in range(len(b_s)):\n",
    "    pre_flop = tune_model(lr, wd, 20, train_preflop, test_preflop, pftr='preflop_last_possible', hidden_nodes=20, batch_size=b_s[z])\n",
    "    ls.append(pre_flop)\n",
    "\n",
    "print(\"PRE FLOP\")\n",
    "print(\"Using learning rate of {} and weight decay of {}:\".format(lr[0], wd[0]))\n",
    "for i in range(len(ls)):\n",
    "    for key, value in ls[i].items():\n",
    "        print(\"     When using batch size of {}, accuracy is {}\".format(b_s[i], value[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "9b807178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOP\n",
      "Using learning rate of 0.01 and weight decay of 0:\n",
      "     When using batch size of 8, accuracy is 85.45\n",
      "     When using batch size of 16, accuracy is 85.75\n",
      "     When using batch size of 32, accuracy is 86.0\n",
      "     When using batch size of 64, accuracy is 85.81\n",
      "     When using batch size of 128, accuracy is 85.54\n",
      "     When using batch size of 256, accuracy is 85.31\n",
      "     When using batch size of 512, accuracy is 84.4\n",
      "     When using batch size of 1024, accuracy is 83.57\n"
     ]
    }
   ],
   "source": [
    "lr = [.01]\n",
    "wd = [0]\n",
    "b_s = [8,16,32,64,128,256,512,1024]\n",
    "\n",
    "ls = []\n",
    "\n",
    "for z in range(len(b_s)):\n",
    "    flop = tune_model(lr, wd, 20, train_flop, test_flop, pftr='flop_last_possible', hidden_nodes=20, batch_size=b_s[z])\n",
    "    ls.append(flop)\n",
    "\n",
    "print(\"FLOP\")\n",
    "print(\"Using learning rate of {} and weight decay of {}:\".format(lr[0], wd[0]))\n",
    "for i in range(len(ls)):\n",
    "    for key, value in ls[i].items():\n",
    "        print(\"     When using batch size of {}, accuracy is {}\".format(b_s[i], value[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "85e4405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TURN\n",
      "Using learning rate of 0.01 and weight decay of 0:\n",
      "     When using batch size of 8, accuracy is 91.67\n",
      "     When using batch size of 16, accuracy is 91.38\n",
      "     When using batch size of 32, accuracy is 91.76\n",
      "     When using batch size of 64, accuracy is 91.63\n",
      "     When using batch size of 128, accuracy is 91.67\n",
      "     When using batch size of 256, accuracy is 91.8\n",
      "     When using batch size of 512, accuracy is 91.46\n",
      "     When using batch size of 1024, accuracy is 90.71\n"
     ]
    }
   ],
   "source": [
    "lr = [.01]\n",
    "wd = [0]\n",
    "b_s = [8,16,32,64,128,256,512,1024]\n",
    "\n",
    "ls = []\n",
    "\n",
    "for z in range(len(b_s)):\n",
    "    turn = tune_model(lr, wd, 40, train_turn, test_turn, pftr='turn_last_possible', hidden_nodes=40, batch_size=b_s[z])\n",
    "    ls.append(turn)\n",
    "    \n",
    "print(\"TURN\")\n",
    "print(\"Using learning rate of {} and weight decay of {}:\".format(lr[0], wd[0]))\n",
    "for i in range(len(ls)):\n",
    "    for key, value in ls[i].items():\n",
    "        print(\"     When using batch size of {}, accuracy is {}\".format(b_s[i], value[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "36c493d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIVER\n",
      "Using learning rate of 0.01 and weight decay of 0:\n",
      "     When using batch size of 8, accuracy is 89.57\n",
      "     When using batch size of 16, accuracy is 89.44\n",
      "     When using batch size of 32, accuracy is 89.9\n",
      "     When using batch size of 64, accuracy is 89.8\n",
      "     When using batch size of 128, accuracy is 89.74\n",
      "     When using batch size of 256, accuracy is 89.44\n",
      "     When using batch size of 512, accuracy is 89.7\n",
      "     When using batch size of 1024, accuracy is 89.62\n"
     ]
    }
   ],
   "source": [
    "lr = [.01]\n",
    "wd = [0]\n",
    "b_s = [8,16,32,64,128,256,512,1024]\n",
    "\n",
    "ls = []\n",
    "\n",
    "for z in range(len(b_s)):\n",
    "    river = tune_model(lr, wd, 40, train_river, test_river, pftr='river_last_possible', hidden_nodes=40, batch_size=b_s[z])\n",
    "    ls.append(river)\n",
    "    \n",
    "print(\"RIVER\")\n",
    "print(\"Using learning rate of {} and weight decay of {}:\".format(lr[0], wd[0]))\n",
    "for i in range(len(ls)):\n",
    "    for key, value in ls[i].items():\n",
    "        print(\"     When using batch size of {}, accuracy is {}\".format(b_s[i], value[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35805b66",
   "metadata": {},
   "source": [
    "# 2) Generating visualization dictionaries - Run all code lines below including the code to pickle the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7aa42d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "flop_nd = get_nodes_dict(10, 100, .01, 0, 20, train_flop, test_flop, pftr='flop_last_possible', batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d963ae13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': (84.82, 84.74), '20': (86.05, 86.11)}"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flop_nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da113a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_nd = get_nodes_dict(10, 100, .01, 0, 40, train_turn, test_turn, pftr='turn_last_possible', batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebf22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "river_nd = get_nodes_dict(10, 100, .01, 0, 40, train_river, test_river, pftr='river_last_possible', batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "river_nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "904c4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "flop_ed = get_epochs_dict(10, 20, .01, 0, 100, train_flop, test_flop, pftr='flop_last_possible', batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7efb5a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': (85.13, 85.17), '20': (85.75, 85.74)}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flop_ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0f35f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_ed = get_epochs_dict(10, 40, .01, 0, 100, train_turn, test_turn, pftr='turn_last_possible', batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "30625514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': (91.15, 91.51), '20': (91.75, 91.45)}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "7ffeb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "river_ed = get_epochs_dict(10, 40, .01, 0, 100, train_river, test_river, pftr='river_last_possible', batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "39717b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': (89.23, 89.53), '20': (89.63, 89.42)}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "river_ed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb777fe",
   "metadata": {},
   "source": [
    "## Running the line below will pickle the dictionaries that will be used for the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9415c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"flop_nd.pickle\", \"wb\") as f: \n",
    "    pickle.dump(flop_nd, f)\n",
    "    \n",
    "with open(\"turn_nd.pickle\", \"wb\") as f: \n",
    "    pickle.dump(turn_nd, f)\n",
    "    \n",
    "with open(\"river_nd.pickle\", \"wb\") as f: \n",
    "    pickle.dump(river_nd, f)\n",
    "    \n",
    "with open(\"flop_ed.pickle\", \"wb\") as f: \n",
    "    pickle.dump(flop_ed, f)\n",
    "    \n",
    "with open(\"turn_ed.pickle\", \"wb\") as f: \n",
    "    pickle.dump(turn_ed, f)\n",
    "    \n",
    "with open(\"river_ed.pickle\", \"wb\") as f: \n",
    "    pickle.dump(river_ed, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject2",
   "language": "python",
   "name": "dlproject2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
