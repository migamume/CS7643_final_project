{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6f8330cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd '/Users/pc/Desktop/Deep Learning/Project Github/CS7643_final_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "810a4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python src/holdup/datapickler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a00fdc",
   "metadata": {},
   "source": [
    "## This is a notebook that creates two dictionaries: d_nodes will provide the accuracy of the preflop, flop, turn and river data set for different hidden nodes and d_epochs will provide the accuracy for different epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66578102",
   "metadata": {},
   "source": [
    "## The code takes alot of time to run. For that reason I have already saved the dictionaries into the following directory: src/holdup/visualizations/json_dictionaries. This notebook should only be used in case somebody wants to regenerate the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d080c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preflop_train_data_size: 30650\n",
      "preflop_test_data_size: 20434\n",
      "flop_train_data_size: 23450\n",
      "flop_test_data_size: 15634\n",
      "turn_train_data_size: 18447\n",
      "turn_test_data_size: 12298\n",
      "river_train_data_size: 16371\n",
      "river_test_data_size: 10915\n"
     ]
    }
   ],
   "source": [
    "from holdup.the_model.autoencoder import Autoencoder\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "from holdup.parser.replayable_hand import ReplayableHand, Streets\n",
    "import functools\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "from holdup.the_model.get_datasets import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "preflop = \"preflop\"\n",
    "flop = \"flop\"\n",
    "turn = \"turn\"\n",
    "river = \"river\"\n",
    "\n",
    "def get_stage(dataset, stage):\n",
    "    if stage == preflop:\n",
    "        return dataset[0]\n",
    "    if stage == flop:\n",
    "        return dataset[1]\n",
    "    if stage == turn:\n",
    "        return dataset[2]\n",
    "    if stage == river:\n",
    "        return dataset[3]\n",
    "\n",
    "def flatten_streets(dataset):\n",
    "    streets = [[], [], [], []]\n",
    "    for logfile in dataset:\n",
    "        for index, street in enumerate(logfile):\n",
    "            streets[index] = streets[index] + street\n",
    "    return streets\n",
    "\n",
    "\n",
    "def get_data(dataset, stage):\n",
    "    flattened_data = flatten_streets(dataset)\n",
    "    stage_data = get_stage(flattened_data, stage)\n",
    "    return [(x[0], x[1][1]) for x in stage_data]\n",
    "\n",
    "\n",
    "with open('last_possible.pickle', 'rb') as last_possible_pickle:\n",
    "    last_possible_dataset = pickle.load(last_possible_pickle)\n",
    "\n",
    "preflop_data = get_data(last_possible_dataset, \"preflop\")\n",
    "flop_data = get_data(last_possible_dataset, \"flop\")\n",
    "turn_data = get_data(last_possible_dataset, \"turn\")\n",
    "river_data = get_data(last_possible_dataset, \"river\")\n",
    "\n",
    "def separate_train_test(street_data):\n",
    "    n_train = int(len(street_data)*0.6)\n",
    "    train_set = street_data[:n_train]\n",
    "    test_set = street_data[n_train:]\n",
    "    return train_set,test_set\n",
    "\n",
    "train_preflop, test_preflop =separate_train_test(preflop_data)\n",
    "print(\"preflop_train_data_size: {}\".format(len(train_preflop)))\n",
    "print(\"preflop_test_data_size: {}\".format(len(test_preflop)))\n",
    "\n",
    "train_flop, test_flop=separate_train_test(flop_data)\n",
    "print(\"flop_train_data_size: {}\".format(len(train_flop)))\n",
    "print(\"flop_test_data_size: {}\".format(len(test_flop)))\n",
    "\n",
    "train_turn, test_turn=separate_train_test(turn_data)\n",
    "print(\"turn_train_data_size: {}\".format(len(train_turn)))\n",
    "print(\"turn_test_data_size: {}\".format(len(test_turn)))\n",
    "\n",
    "train_river, test_river=separate_train_test(river_data)\n",
    "print(\"river_train_data_size: {}\".format(len(train_river)))\n",
    "print(\"river_test_data_size: {}\".format(len(test_river)))\n",
    "\n",
    "\n",
    "# Set the device to use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create an instance of the autoencoder\n",
    "# model = Autoencoder(num_hidden_nodes).to(device)\n",
    "\n",
    "def train2(learning_rate, model, train_loader, num_epochs, wd, pftr='stage_name'):\n",
    "    criterion = nn.CrossEntropyLoss() #changed to cross entropy loss for classification based tasks (semi-supervised)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate, weight_decay=wd)\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            optimizer.zero_grad()\n",
    "            batch_size, _, _ = inputs.size()\n",
    "            inputs = inputs.view(batch_size, -1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "\n",
    "    \n",
    "def quick_test2(model,test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            batch_size, _, _ = inputs.size()\n",
    "            inputs = inputs.view(batch_size, -1)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return int(round((correct/total)*100, 0))\n",
    "\n",
    "\n",
    "def train_and_quick_test(lr, num_hidden_nodes, num_epochs, weight_decay,train_data,test_data, pftr):\n",
    "    # Define the model\n",
    "    model = Autoencoder(num_hidden_nodes).to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=20, shuffle=False)\n",
    "    train2(lr, model, train_loader, num_epochs, weight_decay,pftr)\n",
    "    # Test the model\n",
    "    quick_test(model,test_loader)\n",
    "    \n",
    "\n",
    "###\n",
    "def get_visualization_parameters_nodes(lr, weight_decay,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop, pftr_flop, pftr_turn, pftr_river):\n",
    "    final_dict = {}\n",
    "    for i in range(10,110,10):\n",
    "        \n",
    "        final_dict[str(i)] = []\n",
    "        \n",
    "        model = Autoencoder(i).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_preflop, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_preflop, batch_size=20, shuffle=False)\n",
    "        train2(lr, model, train_loader, 20, weight_decay,pftr_preflop)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "\n",
    "        model = Autoencoder(i).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_flop, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_flop, batch_size=20, shuffle=False)\n",
    "        train2(lr, model, train_loader, 20, weight_decay,pftr_flop)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "\n",
    "        \n",
    "        model = Autoencoder(i).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_turn, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_turn, batch_size=20, shuffle=False)\n",
    "        train2(lr, model, train_loader, 40, weight_decay,pftr_turn)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "        model = Autoencoder(i).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_river, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_river, batch_size=20, shuffle=False)\n",
    "        train2(lr, model, train_loader, 40, weight_decay,pftr_river)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "        final_dict[str(i)] = tuple(final_dict[str(i)])\n",
    "        \n",
    "        print(final_dict)\n",
    "\n",
    "        \n",
    "    \n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def get_visualization_parameters_epochs(lr, weight_decay,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop, pftr_flop, pftr_turn, pftr_river):\n",
    "\n",
    "    final_dict = {}\n",
    "    for i in range(10,70,10):\n",
    "        \n",
    "        final_dict[str(i)] = []\n",
    "        \n",
    "        model = Autoencoder(20).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_preflop, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_preflop, batch_size=20, shuffle=False)\n",
    "        train2(lr, model, train_loader, i, weight_decay,pftr_preflop)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "\n",
    "        model = Autoencoder(20).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_flop, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_flop, batch_size=20, shuffle=False)\n",
    "        train2(lr, model, train_loader, i, weight_decay,pftr_flop)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "        model = Autoencoder(40).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_turn, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_turn, batch_size=20, shuffle=False)\n",
    "        train2(lr, model, train_loader, i, weight_decay,pftr_turn)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "        model = Autoencoder(40).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_river, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_river, batch_size=20, shuffle=False)\n",
    "        train2(lr, model, train_loader, i, weight_decay,pftr_river)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "        final_dict[str(i)] = tuple(final_dict[str(i)])\n",
    "        \n",
    "        print(final_dict)\n",
    "        \n",
    "    \n",
    "    return final_dict\n",
    "\n",
    "def get_visualization_dictionaries(lr, weight_decay,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop, pftr_flop, pftr_turn, pftr_river):\n",
    "    \n",
    "    d_f1 = get_visualization_parameters_nodes(lr, weight_decay,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop, pftr_flop, pftr_turn, pftr_river)\n",
    "    d_f2 = get_visualization_parameters_epochs(lr, weight_decay,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop, pftr_flop, pftr_turn, pftr_river)\n",
    "        \n",
    "    return d_f1, d_f2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94dacf",
   "metadata": {},
   "source": [
    "# Tuning the model\n",
    "\n",
    "## Best Model Parameters: \n",
    "\n",
    "### Learning Rate: .01 \n",
    "### Weight Decay: 0 (as it decreases it improves test accuracy)\n",
    "### Batch Size: 20 (Using a batch size of 20 as default as batch size doesn't have significant impact on model performance)\n",
    "\n",
    "## Code below is used to see the test accuracy obtained by using different learning rates with different weight decays. It can be seen that using a learning rate of .01 gave the best performance out of all of the models and that by decreasing weight decay the model works better and for that reason we opted to using a weight decay of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c36c41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tune_model(learning_rate, weight_decay, num_epochs, train_preflop, test_preflop, pftr, hidden_nodes, batch_size):\n",
    "        lr = {}\n",
    "        for i in range(len(learning_rate)):\n",
    "            lr[str(learning_rate[i])] = []\n",
    "            for j in range(len(weight_decay)):\n",
    "                model = Autoencoder(hidden_nodes).to(device)\n",
    "                train_loader = torch.utils.data.DataLoader(train_flop, batch_size=batch_size, shuffle=True)\n",
    "                test_loader = torch.utils.data.DataLoader(test_flop, batch_size=batch_size, shuffle=False)\n",
    "                train2(learning_rate[i], model, train_loader, num_epochs, weight_decay[j],pftr)\n",
    "                result = quick_test2(model,test_loader)\n",
    "                lr[str(learning_rate[i])].append(result)\n",
    "        return lr\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a6981b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [.0001, .001, .01, .1]\n",
    "wd = [0, .0001, .1]\n",
    "\n",
    "pre_flop = tune_model(lr, wd, 20, train_preflop, test_preflop, pftr='preflop_last_possible', hidden_nodes=20, batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8e9bfe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE FLOP\n",
      "learning rate: 0.0001\n",
      "     weight decay: 0 accuracy: 83\n",
      "     weight decay: 0.0001 accuracy: 83\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.001\n",
      "     weight decay: 0 accuracy: 85\n",
      "     weight decay: 0.0001 accuracy: 84\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.01\n",
      "     weight decay: 0 accuracy: 86\n",
      "     weight decay: 0.0001 accuracy: 84\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.1\n",
      "     weight decay: 0 accuracy: 82\n",
      "     weight decay: 0.0001 accuracy: 77\n",
      "     weight decay: 0.1 accuracy: 58\n"
     ]
    }
   ],
   "source": [
    "pre_flop\n",
    "\n",
    "print(\"PRE FLOP\")\n",
    "for key, value in pre_flop.items():\n",
    "    print(\"learning rate: {}\".format(key))\n",
    "    for j in range(len(value)):\n",
    "        print(\"     weight decay: {}\".format(wd[j]), \"accuracy: {}\".format(value[j]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5079960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [.0001, .001, .01, .1]\n",
    "wd = [0, .0001, .1]\n",
    "\n",
    "flop = tune_model(lr, wd, 20, train_flop, test_flop, pftr='flop_last_possible', hidden_nodes=20, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9b9335fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOP\n",
      "learning rate: 0.0001\n",
      "     weight decay: 0 accuracy: 83\n",
      "     weight decay: 0.0001 accuracy: 83\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.001\n",
      "     weight decay: 0 accuracy: 84\n",
      "     weight decay: 0.0001 accuracy: 84\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.01\n",
      "     weight decay: 0 accuracy: 86\n",
      "     weight decay: 0.0001 accuracy: 84\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.1\n",
      "     weight decay: 0 accuracy: 84\n",
      "     weight decay: 0.0001 accuracy: 77\n",
      "     weight decay: 0.1 accuracy: 58\n"
     ]
    }
   ],
   "source": [
    "print(\"FLOP\")\n",
    "for key, value in flop.items():\n",
    "    print(\"learning rate: {}\".format(key))\n",
    "    for j in range(len(value)):\n",
    "        print(\"     weight decay: {}\".format(wd[j]), \"accuracy: {}\".format(value[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "71a0419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [.0001, .001, .01, .1]\n",
    "wd = [0, .0001, .1]\n",
    "\n",
    "turn = tune_model(lr, wd, 40, train_turn, test_turn, pftr='turn_last_possible', hidden_nodes=40, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e494acd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TURN\n",
      "learning rate: 0.0001\n",
      "     weight decay: 0 accuracy: 83\n",
      "     weight decay: 0.0001 accuracy: 83\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.001\n",
      "     weight decay: 0 accuracy: 87\n",
      "     weight decay: 0.0001 accuracy: 85\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.01\n",
      "     weight decay: 0 accuracy: 87\n",
      "     weight decay: 0.0001 accuracy: 84\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.1\n",
      "     weight decay: 0 accuracy: 83\n",
      "     weight decay: 0.0001 accuracy: 80\n",
      "     weight decay: 0.1 accuracy: 58\n"
     ]
    }
   ],
   "source": [
    "print(\"TURN\")\n",
    "for key, value in turn.items():\n",
    "    print(\"learning rate: {}\".format(key))\n",
    "    for j in range(len(value)):\n",
    "        print(\"     weight decay: {}\".format(wd[j]), \"accuracy: {}\".format(value[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6a6f168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [.0001, .001, .01, .1]\n",
    "wd = [0, .0001, .1]\n",
    "\n",
    "river = tune_model(lr, wd, 40, train_river, test_river, pftr='river_last_possible', hidden_nodes=40, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8c9f084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIVER\n",
      "learning rate: 0.0001\n",
      "     weight decay: 0 accuracy: 83\n",
      "     weight decay: 0.0001 accuracy: 83\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.001\n",
      "     weight decay: 0 accuracy: 87\n",
      "     weight decay: 0.0001 accuracy: 84\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.01\n",
      "     weight decay: 0 accuracy: 87\n",
      "     weight decay: 0.0001 accuracy: 85\n",
      "     weight decay: 0.1 accuracy: 58\n",
      "learning rate: 0.1\n",
      "     weight decay: 0 accuracy: 83\n",
      "     weight decay: 0.0001 accuracy: 81\n",
      "     weight decay: 0.1 accuracy: 58\n"
     ]
    }
   ],
   "source": [
    "print(\"RIVER\")\n",
    "for key, value in river.items():\n",
    "    print(\"learning rate: {}\".format(key))\n",
    "    for j in range(len(value)):\n",
    "        print(\"     weight decay: {}\".format(wd[j]), \"accuracy: {}\".format(value[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c442d0",
   "metadata": {},
   "source": [
    "## Code Below is used to demonstrate how changing the batch size does not make significant change to improving the test accuracy of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d0e2d92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE FLOP\n",
      "Using learning rate of 0.01 and weight decay of 0:\n",
      "     When using batch size of 8, accuracy is 86\n",
      "     When using batch size of 16, accuracy is 87\n",
      "     When using batch size of 32, accuracy is 87\n",
      "     When using batch size of 64, accuracy is 87\n",
      "     When using batch size of 128, accuracy is 86\n",
      "     When using batch size of 256, accuracy is 86\n",
      "     When using batch size of 512, accuracy is 85\n",
      "     When using batch size of 1024, accuracy is 84\n"
     ]
    }
   ],
   "source": [
    "lr = [.01]\n",
    "wd = [0]\n",
    "b_s = [8,16,32,64,128,256,512,1024]\n",
    "\n",
    "ls = []\n",
    "\n",
    "for z in range(len(b_s)):\n",
    "    pre_flop = tune_model(lr, wd, 20, train_preflop, test_preflop, pftr='preflop_last_possible', hidden_nodes=20, batch_size=b_s[z])\n",
    "    ls.append(pre_flop)\n",
    "\n",
    "print(\"PRE FLOP\")\n",
    "print(\"Using learning rate of {} and weight decay of {}:\".format(lr[0], wd[0]))\n",
    "for i in range(len(ls)):\n",
    "    for key, value in ls[i].items():\n",
    "        print(\"     When using batch size of {}, accuracy is {}\".format(b_s[i], value[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7f01e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOP\n",
      "Using learning rate of 0.01 and weight decay of 0:\n",
      "     When using batch size of 8, accuracy is 86\n",
      "     When using batch size of 16, accuracy is 87\n",
      "     When using batch size of 32, accuracy is 87\n",
      "     When using batch size of 64, accuracy is 86\n",
      "     When using batch size of 128, accuracy is 87\n",
      "     When using batch size of 256, accuracy is 86\n",
      "     When using batch size of 512, accuracy is 86\n",
      "     When using batch size of 1024, accuracy is 84\n"
     ]
    }
   ],
   "source": [
    "lr = [.01]\n",
    "wd = [0]\n",
    "b_s = [8,16,32,64,128,256,512,1024]\n",
    "\n",
    "ls = []\n",
    "\n",
    "for z in range(len(b_s)):\n",
    "    flop = tune_model(lr, wd, 20, train_flop, test_flop, pftr='flop_last_possible', hidden_nodes=20, batch_size=b_s[z])\n",
    "    ls.append(flop)\n",
    "\n",
    "print(\"FLOP\")\n",
    "print(\"Using learning rate of {} and weight decay of {}:\".format(lr[0], wd[0]))\n",
    "for i in range(len(ls)):\n",
    "    for key, value in ls[i].items():\n",
    "        print(\"     When using batch size of {}, accuracy is {}\".format(b_s[i], value[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5069e5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TURN\n",
      "Using learning rate of 0.01 and weight decay of 0:\n",
      "     When using batch size of 8, accuracy is 86\n",
      "     When using batch size of 16, accuracy is 87\n",
      "     When using batch size of 32, accuracy is 87\n",
      "     When using batch size of 64, accuracy is 87\n",
      "     When using batch size of 128, accuracy is 87\n",
      "     When using batch size of 256, accuracy is 87\n",
      "     When using batch size of 512, accuracy is 87\n",
      "     When using batch size of 1024, accuracy is 87\n"
     ]
    }
   ],
   "source": [
    "lr = [.01]\n",
    "wd = [0]\n",
    "b_s = [8,16,32,64,128,256,512,1024]\n",
    "\n",
    "ls = []\n",
    "\n",
    "for z in range(len(b_s)):\n",
    "    turn = tune_model(lr, wd, 40, train_turn, test_turn, pftr='turn_last_possible', hidden_nodes=40, batch_size=b_s[z])\n",
    "    ls.append(turn)\n",
    "    \n",
    "print(\"TURN\")\n",
    "print(\"Using learning rate of {} and weight decay of {}:\".format(lr[0], wd[0]))\n",
    "for i in range(len(ls)):\n",
    "    for key, value in ls[i].items():\n",
    "        print(\"     When using batch size of {}, accuracy is {}\".format(b_s[i], value[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "46e58ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIVER\n",
      "Using learning rate of 0.01 and weight decay of 0:\n",
      "     When using batch size of 8, accuracy is 87\n",
      "     When using batch size of 16, accuracy is 87\n",
      "     When using batch size of 32, accuracy is 87\n",
      "     When using batch size of 64, accuracy is 87\n",
      "     When using batch size of 128, accuracy is 87\n",
      "     When using batch size of 256, accuracy is 87\n",
      "     When using batch size of 512, accuracy is 87\n",
      "     When using batch size of 1024, accuracy is 86\n"
     ]
    }
   ],
   "source": [
    "lr = [.01]\n",
    "wd = [0]\n",
    "b_s = [8,16,32,64,128,256,512,1024]\n",
    "\n",
    "ls = []\n",
    "\n",
    "for z in range(len(b_s)):\n",
    "    river = tune_model(lr, wd, 40, train_river, test_river, pftr='river_last_possible', hidden_nodes=40, batch_size=b_s[z])\n",
    "    ls.append(river)\n",
    "    \n",
    "print(\"RIVER\")\n",
    "print(\"Using learning rate of {} and weight decay of {}:\".format(lr[0], wd[0]))\n",
    "for i in range(len(ls)):\n",
    "    for key, value in ls[i].items():\n",
    "        print(\"     When using batch size of {}, accuracy is {}\".format(b_s[i], value[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35805b66",
   "metadata": {},
   "source": [
    "## Running the code line below will generate two dictionaries: d_nodes and d_epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bd277204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10': (83, 86, 91, 89)}\n",
      "{'10': (83, 86, 91, 89), '20': (83, 86, 92, 89)}\n",
      "{'10': (83, 86, 91, 89), '20': (83, 86, 92, 89), '30': (83, 87, 92, 89)}\n",
      "{'10': (83, 86, 91, 89), '20': (83, 86, 92, 89), '30': (83, 87, 92, 89), '40': (83, 87, 92, 90)}\n",
      "{'10': (83, 86, 91, 89), '20': (83, 86, 92, 89), '30': (83, 87, 92, 89), '40': (83, 87, 92, 90), '50': (83, 87, 92, 90)}\n",
      "{'10': (83, 86, 91, 89), '20': (83, 86, 92, 89), '30': (83, 87, 92, 89), '40': (83, 87, 92, 90), '50': (83, 87, 92, 90), '60': (83, 87, 92, 89)}\n",
      "{'10': (83, 86, 91, 89), '20': (83, 86, 92, 89), '30': (83, 87, 92, 89), '40': (83, 87, 92, 90), '50': (83, 87, 92, 90), '60': (83, 87, 92, 89), '70': (83, 87, 92, 90)}\n",
      "{'10': (83, 86, 91, 89), '20': (83, 86, 92, 89), '30': (83, 87, 92, 89), '40': (83, 87, 92, 90), '50': (83, 87, 92, 90), '60': (83, 87, 92, 89), '70': (83, 87, 92, 90), '80': (83, 87, 92, 90)}\n",
      "{'10': (83, 86, 91, 89), '20': (83, 86, 92, 89), '30': (83, 87, 92, 89), '40': (83, 87, 92, 90), '50': (83, 87, 92, 90), '60': (83, 87, 92, 89), '70': (83, 87, 92, 90), '80': (83, 87, 92, 90), '90': (84, 87, 92, 90)}\n",
      "{'10': (83, 86, 91, 89), '20': (83, 86, 92, 89), '30': (83, 87, 92, 89), '40': (83, 87, 92, 90), '50': (83, 87, 92, 90), '60': (83, 87, 92, 89), '70': (83, 87, 92, 90), '80': (83, 87, 92, 90), '90': (84, 87, 92, 90), '100': (83, 87, 92, 90)}\n",
      "{'10': (83, 86, 92, 90)}\n",
      "{'10': (83, 86, 92, 90), '20': (83, 86, 92, 90)}\n",
      "{'10': (83, 86, 92, 90), '20': (83, 86, 92, 90), '30': (83, 86, 93, 90)}\n",
      "{'10': (83, 86, 92, 90), '20': (83, 86, 92, 90), '30': (83, 86, 93, 90), '40': (83, 87, 92, 90)}\n",
      "{'10': (83, 86, 92, 90), '20': (83, 86, 92, 90), '30': (83, 86, 93, 90), '40': (83, 87, 92, 90), '50': (83, 86, 92, 90)}\n",
      "{'10': (83, 86, 92, 90), '20': (83, 86, 92, 90), '30': (83, 86, 93, 90), '40': (83, 87, 92, 90), '50': (83, 86, 92, 90), '60': (83, 87, 92, 90)}\n"
     ]
    }
   ],
   "source": [
    "d_nodes2, d_epochs2 = get_visualization_dictionaries(.01, 0,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop='preflop_last_possible', pftr_flop='flop_last_possible', pftr_turn='turn_last_possible', pftr_river='river_last_possible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e3365f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': (83, 86, 91, 89),\n",
       " '20': (83, 86, 92, 89),\n",
       " '30': (83, 87, 92, 89),\n",
       " '40': (83, 87, 92, 90),\n",
       " '50': (83, 87, 92, 90),\n",
       " '60': (83, 87, 92, 89),\n",
       " '70': (83, 87, 92, 90),\n",
       " '80': (83, 87, 92, 90),\n",
       " '90': (84, 87, 92, 90),\n",
       " '100': (83, 87, 92, 90)}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_nodes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "da580572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': (83, 86, 92, 90),\n",
       " '20': (83, 86, 92, 90),\n",
       " '30': (83, 86, 93, 90),\n",
       " '40': (83, 87, 92, 90),\n",
       " '50': (83, 86, 92, 90),\n",
       " '60': (83, 87, 92, 90)}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_epochs2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb777fe",
   "metadata": {},
   "source": [
    "## Running the line below will export the dictionaires as json files to src/holdup/visualizations/json_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9415c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(r'src/holdup/visualizations/json_dictionaries/d_nodes.json', 'w') as f:\n",
    "    json.dump(d_nodes2, f)\n",
    "\n",
    "with open(r'src/holdup/visualizations/json_dictionaries/d_epochs.json', 'w') as f:\n",
    "    json.dump(d_epochs2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e96cdb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb5afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677439b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d88087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a54dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject2",
   "language": "python",
   "name": "dlproject2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
