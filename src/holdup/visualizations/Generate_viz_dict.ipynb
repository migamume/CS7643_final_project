{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f8330cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd '/Users/pc/Desktop/Deep Learning/Project Github/CS7643_final_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "810a4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python src/holdup/datapickler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a00fdc",
   "metadata": {},
   "source": [
    "## This is a notebook that creates two dictionaries: d_nodes will provide the accuracy of the preflop, flop, turn and river data set for different hidden nodes and d_epochs will provide the accuracy for different epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66578102",
   "metadata": {},
   "source": [
    "## The code takes alot of time to run. For that reason I have already saved the dictionaries into the following directory: src/holdup/visualizations/json_dictionaries. This notebook should only be used in case somebody wants to regenerate the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d080c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preflop_train_data_size: 30650\n",
      "preflop_test_data_size: 20434\n",
      "flop_train_data_size: 23450\n",
      "flop_test_data_size: 15634\n",
      "turn_train_data_size: 18447\n",
      "turn_test_data_size: 12298\n",
      "river_train_data_size: 16371\n",
      "river_test_data_size: 10915\n"
     ]
    }
   ],
   "source": [
    "from holdup.the_model.autoencoder import Autoencoder\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "from holdup.parser.replayable_hand import ReplayableHand, Streets\n",
    "import functools\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "from holdup.the_model.get_datasets import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "preflop = \"preflop\"\n",
    "flop = \"flop\"\n",
    "turn = \"turn\"\n",
    "river = \"river\"\n",
    "\n",
    "def get_stage(dataset, stage):\n",
    "    if stage == preflop:\n",
    "        return dataset[0]\n",
    "    if stage == flop:\n",
    "        return dataset[1]\n",
    "    if stage == turn:\n",
    "        return dataset[2]\n",
    "    if stage == river:\n",
    "        return dataset[3]\n",
    "\n",
    "def flatten_streets(dataset):\n",
    "    streets = [[], [], [], []]\n",
    "    for logfile in dataset:\n",
    "        for index, street in enumerate(logfile):\n",
    "            streets[index] = streets[index] + street\n",
    "    return streets\n",
    "\n",
    "\n",
    "def get_data(dataset, stage):\n",
    "    flattened_data = flatten_streets(dataset)\n",
    "    stage_data = get_stage(flattened_data, stage)\n",
    "    return [(x[0], x[1][1]) for x in stage_data]\n",
    "\n",
    "\n",
    "with open('last_possible.pickle', 'rb') as last_possible_pickle:\n",
    "    last_possible_dataset = pickle.load(last_possible_pickle)\n",
    "\n",
    "preflop_data = get_data(last_possible_dataset, \"preflop\")\n",
    "flop_data = get_data(last_possible_dataset, \"flop\")\n",
    "turn_data = get_data(last_possible_dataset, \"turn\")\n",
    "river_data = get_data(last_possible_dataset, \"river\")\n",
    "\n",
    "def separate_train_test(street_data):\n",
    "    n_train = int(len(street_data)*0.6)\n",
    "    train_set = street_data[:n_train]\n",
    "    test_set = street_data[n_train:]\n",
    "    return train_set,test_set\n",
    "\n",
    "train_preflop, test_preflop =separate_train_test(preflop_data)\n",
    "print(\"preflop_train_data_size: {}\".format(len(train_preflop)))\n",
    "print(\"preflop_test_data_size: {}\".format(len(test_preflop)))\n",
    "\n",
    "train_flop, test_flop=separate_train_test(flop_data)\n",
    "print(\"flop_train_data_size: {}\".format(len(train_flop)))\n",
    "print(\"flop_test_data_size: {}\".format(len(test_flop)))\n",
    "\n",
    "train_turn, test_turn=separate_train_test(turn_data)\n",
    "print(\"turn_train_data_size: {}\".format(len(train_turn)))\n",
    "print(\"turn_test_data_size: {}\".format(len(test_turn)))\n",
    "\n",
    "train_river, test_river=separate_train_test(river_data)\n",
    "print(\"river_train_data_size: {}\".format(len(train_river)))\n",
    "print(\"river_test_data_size: {}\".format(len(test_river)))\n",
    "\n",
    "\n",
    "# Set the device to use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create an instance of the autoencoder\n",
    "# model = Autoencoder(num_hidden_nodes).to(device)\n",
    "\n",
    "def train(model, train_loader, num_epochs, weight_decay, pftr='stage_name'):\n",
    "    criterion = nn.CrossEntropyLoss() #changed to cross entropy loss for classification based tasks (semi-supervised)\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            optimizer.zero_grad()\n",
    "            batch_size, _, _ = inputs.size()\n",
    "            inputs = inputs.view(batch_size, -1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "\n",
    "    \n",
    "def quick_test2(model,test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            batch_size, _, _ = inputs.size()\n",
    "            inputs = inputs.view(batch_size, -1)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return int(round((correct/total)*100, 0))\n",
    "\n",
    "\n",
    "def train_and_quick_test(num_hidden_nodes, num_epochs, weight_decay,train_data,test_data, pftr):\n",
    "    # Define the model\n",
    "    model = Autoencoder(num_hidden_nodes).to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=20, shuffle=False)\n",
    "    train(model, train_loader, num_epochs, weight_decay,pftr)\n",
    "    # Test the model\n",
    "    quick_test(model,test_loader)\n",
    "    \n",
    "\n",
    "###\n",
    "def get_visualization_parameters_nodes(weight_decay,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop, pftr_flop, pftr_turn, pftr_river):\n",
    "    final_dict = {}\n",
    "    for i in range(10,110,10):\n",
    "        \n",
    "        final_dict[str(i)] = []\n",
    "        \n",
    "        model = Autoencoder(i).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_preflop, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_preflop, batch_size=20, shuffle=False)\n",
    "        train(model, train_loader, 20, weight_decay,pftr_preflop)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "\n",
    "        model = Autoencoder(i).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_flop, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_flop, batch_size=20, shuffle=False)\n",
    "        train(model, train_loader, 20, weight_decay,pftr_flop)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "\n",
    "        \n",
    "        model = Autoencoder(i).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_turn, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_turn, batch_size=20, shuffle=False)\n",
    "        train(model, train_loader, 40, weight_decay,pftr_turn)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "        model = Autoencoder(i).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_river, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_river, batch_size=20, shuffle=False)\n",
    "        train(model, train_loader, 40, weight_decay,pftr_river)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "        final_dict[str(i)] = tuple(final_dict[str(i)])\n",
    "        \n",
    "        print(final_dict)\n",
    "\n",
    "        \n",
    "    \n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def get_visualization_parameters_epochs(weight_decay,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop, pftr_flop, pftr_turn, pftr_river):\n",
    "\n",
    "    final_dict = {}\n",
    "    for i in range(10,70,10):\n",
    "        \n",
    "        final_dict[str(i)] = []\n",
    "        \n",
    "        model = Autoencoder(20).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_preflop, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_preflop, batch_size=20, shuffle=False)\n",
    "        train(model, train_loader, i, weight_decay,pftr_preflop)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "\n",
    "        model = Autoencoder(20).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_flop, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_flop, batch_size=20, shuffle=False)\n",
    "        train(model, train_loader, i, weight_decay,pftr_flop)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "        model = Autoencoder(40).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_turn, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_turn, batch_size=20, shuffle=False)\n",
    "        train(model, train_loader, i, weight_decay,pftr_turn)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "        model = Autoencoder(40).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_river, batch_size=20, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_river, batch_size=20, shuffle=False)\n",
    "        train(model, train_loader, i, weight_decay,pftr_river)\n",
    "        result = quick_test2(model,test_loader)\n",
    "        final_dict[str(i)].append(result)\n",
    "        \n",
    "        final_dict[str(i)] = tuple(final_dict[str(i)])\n",
    "        \n",
    "        print(final_dict)\n",
    "        \n",
    "    \n",
    "    return final_dict\n",
    "\n",
    "def get_visualization_dictionaries(weight_decay,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop, pftr_flop, pftr_turn, pftr_river):\n",
    "    \n",
    "    d_f1 = get_visualization_parameters_nodes(weight_decay,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop, pftr_flop, pftr_turn, pftr_river)\n",
    "    d_f2 = get_visualization_parameters_epochs(weight_decay,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop, pftr_flop, pftr_turn, pftr_river)\n",
    "        \n",
    "    return d_f1, d_f2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35805b66",
   "metadata": {},
   "source": [
    "## Running the code line below will generate two dictionaries: d_nodes and d_epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd277204",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_nodes, d_epochs = get_visualization_dictionaries(.001,train_preflop, train_flop, train_turn, train_river, test_preflop, test_flop, test_turn, test_river, pftr_preflop='preflop_last_possible', pftr_flop='flop_last_possible', pftr_turn='turn_last_possible', pftr_river='river_last_possible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3365f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': (80, 83, 90, 88),\n",
       " '20': (79, 83, 90, 87),\n",
       " '30': (78, 82, 90, 87),\n",
       " '40': (80, 82, 90, 87),\n",
       " '50': (79, 83, 91, 87),\n",
       " '60': (79, 80, 90, 87),\n",
       " '70': (80, 82, 90, 87),\n",
       " '80': (78, 83, 90, 87),\n",
       " '90': (80, 82, 90, 87),\n",
       " '100': (78, 82, 90, 87)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da580572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': (80, 83, 90, 87),\n",
       " '20': (79, 83, 90, 87),\n",
       " '30': (79, 83, 90, 87),\n",
       " '40': (80, 83, 89, 87),\n",
       " '50': (79, 83, 90, 88),\n",
       " '60': (78, 82, 90, 88)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb777fe",
   "metadata": {},
   "source": [
    "## Running the line below will export the dictionaires as json files to src/holdup/visualizations/json_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9415c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(r'src/holdup/visualizations/json_dictionaries/d_nodes.json', 'w') as f:\n",
    "    json.dump(d_nodes, f)\n",
    "\n",
    "with open(r'src/holdup/visualizations/json_dictionaries/d_epochs.json', 'w') as f:\n",
    "    json.dump(d_epochs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e96cdb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb5afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677439b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d88087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a54dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject2",
   "language": "python",
   "name": "dlproject2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
